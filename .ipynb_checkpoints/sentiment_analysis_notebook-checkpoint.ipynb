{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52cd8a10-ce4d-4446-96a4-a5c2f3d5b1c3",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Using the Natural Language Toolkit (NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54782989-5f45-4eb3-a34a-cbe795ce348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import re, string\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('twitter_samples')\n",
    "# nltk.download('punkt') # helps you tokenize words and sentences\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d48c9ec9-83d9-4243-8284-c56fca610212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking Samples\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6b47c1e-5c6f-4024-87d8-ac0313d58aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87fc7997-2c8f-417a-a062-0fab1952c906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(tweet_tokens[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a419152-5824-466c-b38e-20a25d4c8102",
   "metadata": {},
   "source": [
    "#  Normalizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0df4dc9-9849-458f-88a7-9ef0ae47a0b4",
   "metadata": {},
   "source": [
    " Normalization in NLP is the process of converting a word to its canonical form.\n",
    "\n",
    "Normalization helps group together words with the same meaning but different forms. Without normalization, “ran”, “runs”, and “running” would be treated as different words, even though you may want them to be treated as the same word. In this section, you explore stemming and lemmatization, which are two popular techniques of normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b251d71c-9bb0-4233-8aaa-30c62355840c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#FollowFriday', 'JJ'), ('@France_Inte', 'NNP'), ('@PKuchly57', 'NNP'), ('@Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':)', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tag(tweet_tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ca0c97-6099-4d58-bf34-4216537e8e17",
   "metadata": {},
   "source": [
    "From the list of tags, here is the list of the most common items and their meaning:\n",
    "\n",
    "* NNP: Noun, proper, singular\n",
    "* NN: Noun, common, singular or mass\n",
    "* IN: Preposition or conjunction, subordinating\n",
    "* VBG: Verb, gerund or present participle\n",
    "* VBN: Verb, past participle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6263e8c0-4939-4005-a45a-23242aa8a2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'be', 'top', 'engage', 'member', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d087fd2-d35b-4663-b792-76a75949e7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FollowFriday   --> #FollowFriday \n",
      "@France_Inte    --> @France_Inte \n",
      "@PKuchly57      --> @PKuchly57 \n",
      "@Milipol_Paris  --> @Milipol_Paris \n",
      "for             --> for        \n",
      "being           --> be         \n",
      "top             --> top        \n",
      "engaged         --> engage     \n",
      "members         --> member     \n",
      "in              --> in         \n",
      "my              --> my         \n",
      "community       --> community  \n",
      "this            --> this       \n",
      "week            --> week       \n",
      ":)              --> :)         \n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(lemmatize_sentence(tweet_tokens[0]),tweet_tokens[0]):\n",
    "    print(f'{j:15} --> {i:10} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774e57a6-968b-42b1-a21b-31af4f9ef55c",
   "metadata": {},
   "source": [
    "# Removing Noise from the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b64ffa-81cb-45dd-8a3e-1529101c4e6d",
   "metadata": {},
   "source": [
    "Remove noise from the dataset. Noise is any part of the text that does not add meaning or information to data.\n",
    "\n",
    "**Removing Hyperlinks/Twitter handles/Punctuations/Special characters/Stopwords**\n",
    "\n",
    "Noise is specific to each project, so what constitutes noise in one project may not be in a different project. For instance, the most common words in a language are called stop words. Some examples of stop words are “is”, “the”, and “a”. They are generally irrelevant when processing language, unless a specific use case warrants their inclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d773ffd-c0af-436a-9bf3-acd837a5c5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e571842-85af-4567-9efa-8d7f7e762a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(remove_noise(tweet_tokens[0], stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79e902a6-51eb-4f51-8836-15f1b6ea5058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0ea8775-1244-41bc-bcfa-495bef52805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_cleaned_tokens_list = []\n",
    "negative_cleaned_tokens_list = []\n",
    "\n",
    "for tokens in positive_tweet_tokens:\n",
    "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "for tokens in negative_tweet_tokens:\n",
    "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113ec72a-6769-4ff1-892f-7238071edead",
   "metadata": {},
   "source": [
    "Look at the Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac2f3b37-034c-40dd-8923-173a52beb0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n",
      "['dang', 'rad', '#fanart', ':d']\n"
     ]
    }
   ],
   "source": [
    "print(positive_tweet_tokens[500])\n",
    "print(positive_cleaned_tokens_list[500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056aa4e1-4b71-4608-9cab-f4efc1713b9d",
   "metadata": {},
   "source": [
    "#  Determining Word Density"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37fec4a-9488-4355-bd65-089b50cff1fb",
   "metadata": {},
   "source": [
    "Analysis on textual data is to take out the word frequency. A single tweet is too small of an entity to find out the distribution of words, hence, the analysis of the frequency of words would be done on all positive tweets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b96d18dd-cfa4-4a7c-b97e-0fd14193244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab5cf25b-354d-41c9-b538-724ec99b2319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n"
     ]
    }
   ],
   "source": [
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "print(freq_dist_pos.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f1714e-7e78-4c0f-91eb-f728dd33dcab",
   "metadata": {},
   "source": [
    "# Preparing Data for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "25597651-90c2-49c5-95a2-94e6589ff5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c620b26-170d-4844-b5be-7e927175db5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                     for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377d673c-e264-4b01-bb1e-6c483770f7c1",
   "metadata": {},
   "source": [
    "# Building and Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "794467e4-c8ab-4e4d-b76e-bf5ea1ea453d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.9956666666666667\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2043.4 : 1.0\n",
      "                      :) = True           Positi : Negati =   1006.2 : 1.0\n",
      "                     sad = True           Negati : Positi =     24.6 : 1.0\n",
      "                     bam = True           Positi : Negati =     24.0 : 1.0\n",
      "                followed = True           Negati : Positi =     23.3 : 1.0\n",
      "                follower = True           Positi : Negati =     22.2 : 1.0\n",
      "                    sick = True           Negati : Positi =     20.7 : 1.0\n",
      "                     x15 = True           Negati : Positi =     17.4 : 1.0\n",
      "                    glad = True           Positi : Negati =     17.3 : 1.0\n",
      "                   enjoy = True           Positi : Negati =     15.2 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf0144a-c491-4756-8682-51c7060c9540",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "843131cf-2c0c-46e6-aad2-07ad5d69c638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n",
    "# custom_tweet = 'Congrats #SportStar on your 7th best goal from last season winning goal of the year :) #Baller #Topbin #oneofmanyworldies'\n",
    "\n",
    "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2ca3f3-5de7-46fb-b904-3b0f63331056",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "df3856d9-5f1f-4cc0-8a21-7cc5afda94de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('saved_model/nltk_sentiment_classifier.pickle', 'wb')\n",
    "pickle.dump(classifier, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "51af1a83-2069-48c9-a46e-e62b095686a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading model\n",
    "# f = open('saved_model/nltk_sentiment_classifier.pickle', 'rb')\n",
    "# classifier = pickle.load(f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0912eb5f-8483-468a-96b7-c16eaa78c587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbc3ed9-5f86-4e8a-8be8-1f2e16867b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce13e307-f051-4c54-abce-79b5530cca79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_new",
   "language": "python",
   "name": "tf_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
